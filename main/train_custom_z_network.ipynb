{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78068167-8013-4178-809c-17f03945cec6",
   "metadata": {},
   "source": [
    "# IMPORTS DE LIBRAIRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e9a9c-e0a0-4c5a-98cf-ca20d6617c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d02d31-7e3d-4566-9071-4187b20ae959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(\"torch version           : \", torch.__version__)\n",
    "print(\"torch cuda version      : \", torch.version.cuda)\n",
    "print(\"torch.cuda.is_available : \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5074e79-12d1-42c5-8f4a-58e841e8c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "print(\"detectron2 version : \", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985c4d5-3a69-42a1-a665-161c9ca823bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor, DefaultTrainer, launch\n",
    "from detectron2.config import get_cfg, get_stack_cell_config\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data.datasets import get_dicts\n",
    "from detectron2.modeling import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66562ae0-d3bd-442b-ae9d-c22e73fce396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json, cv2, random, glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0190a3c-2391-446d-bb3b-e11657502818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27893f8-47fe-406a-b4a8-3eef16e0a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imBGRshow(img):\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b780c-40af-4be5-a429-a56b28ea861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imRGBshow(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a707c0c-385d-46f7-82a7-21388eb87864",
   "metadata": {},
   "source": [
    "# REGISTER LES IMAGES\n",
    "## /!\\ CHANGE THE DATA PATH ACCORDINGLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925a8ac-e943-42aa-bc9c-d79f544ced5b",
   "metadata": {},
   "source": [
    "NB : Les classes sont les suivantes :\n",
    "- 0 : Cellule intacte et nette   (Intact_Sharp)\n",
    "- 1 : Cellule intacte et floue   (Intact_Blurry)\n",
    "- 2 : Cellule explosée et nette  (Broken_Sharp)\n",
    "- 3 : Cellule explosée et floue  (Broken_Blurry)\n",
    "\n",
    "Pour seulement considérer les cellules nettes, utiliser :\n",
    "classes = {'Intact_Sharp':0, 'Broken_Sharp':2}\n",
    "\n",
    "Pour considérer tous les types de cellules, utiliser :\n",
    "classes = {'Intact_Sharp':0,'Intact_Blurry':1,'Broken_Sharp':2,'Broken_Blurry':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4d5e4-8a23-4ab7-b2e0-bf28ec49c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'Intact_Sharp':0, 'Broken_Sharp':2}\n",
    "#classes = {'Intact_Sharp':0,'Intact_Blurry':1,'Broken_Sharp':2,'Broken_Blurry':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415477bb-1ce2-453c-9c8d-e7f9904c1d24",
   "metadata": {},
   "source": [
    "## DATASET\n",
    "De combien de images ? soit combien de stacks?\n",
    "Le dataset est séparé en 3 jeux de données : \n",
    "- 60%    => Entraînement\n",
    "- 20%    => Validation\n",
    "- 20%    => Test  \n",
    "\n",
    "\n",
    "Les données doivent être rangées dans la structure suivante de fichiers. La variable data_path définie dans la variable suivante doit indiquer l'emplacement du dossier Cross-val.  \n",
    "/!\\ ATTENTION, ce chemin est à adapter.  \n",
    "└── Cross-val  \n",
    "&emsp;&emsp;&emsp;   ├── Xval0  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   ├── images  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   └── labels  \n",
    "&emsp;&emsp;&emsp;   ├── Xval1  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   ├── images  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   └── labels  \n",
    "&emsp;&emsp;&emsp;   ├── Xval2  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   ├── images  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   └── labels  \n",
    "&emsp;&emsp;&emsp;   ├── Xval3  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   ├── images  \n",
    "&emsp;&emsp;&emsp; |&emsp;&emsp;   └── labels  \n",
    "&emsp;&emsp;&emsp;   └── Xval4  \n",
    "&emsp;&emsp;&emsp; &emsp;&emsp;   ├── images  \n",
    "&emsp;&emsp;&emsp; &emsp;&emsp;   └── labels  \n",
    "\n",
    "Comme son nom l'indique, cette séparation est réalisée afin de pouvoir faire de la validation croisée (cross-validation). Pour des raisons écologiques et de durée d'entraînement, nous n'avons pas tiré profit de cette possibilité, mais il est important de noter qu'elle est facilement implémetable au besoin.  \n",
    "Un indice indique quelles parties du dataset seront associées avec quel jeu de données (entraînement, validation ou test). Pour réaliser de la validation croisée, il faudra réaliser l'entrainement pour des indices variant de 0 à 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d51a4f-e54f-4567-816e-4fe855f97c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/projects/INSA-Image/B01/Data/'\n",
    "cross_val_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b5e97-ef06-4482-bd1a-324ffdee78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modes must have the correct string associated in order to perform the proper operation\n",
    "mode_train = 'train'\n",
    "mode_valid = 'val'\n",
    "mode_test  = 'test'\n",
    "\n",
    "# By default in our architecture. To use custom names, an override of these names must happen during the configuration (see next section)\n",
    "dataset_name_train = 'train'\n",
    "dataset_name_valid = 'val'\n",
    "dataset_name_test  = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376f531-5abc-446a-bc39-f2d6f3177121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the datasets\n",
    "DatasetCatalog.register(dataset_name_train, lambda: get_dicts(data_path, mode_train, cross_val_idx, classes, dataset_name_train))\n",
    "DatasetCatalog.register(dataset_name_valid, lambda: get_dicts(data_path, mode_valid, cross_val_idx, classes, dataset_name_valid))\n",
    "DatasetCatalog.register(dataset_name_test,  lambda: get_dicts(data_path, mode_test,  cross_val_idx, classes, dataset_name_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c5b2f-b015-4648-9b03-67990b0e6469",
   "metadata": {},
   "source": [
    "# AFFICHAGE DE QUELQUES IMAGES AVEC LEUR SEGMENTATION MANUELLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712503ad-59c0-4e77-8a58-97258d28052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metadata = MetadataCatalog.get(dataset_name_valid)\n",
    "valid_dataset_dicts = DatasetCatalog.get(dataset_name_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffda710-424f-4787-a5b6-ac7d72fb80a6",
   "metadata": {},
   "source": [
    "La cellule suivante permet par défaut d'afficher 2 images ainsi que leur segmentation. Ces 2 images sont tirées au hasard dans le dataset de validation.  \n",
    "  \n",
    "Remarque : Si les seules classes représentées sont les cellules nettes, il est très peu probable que des segmentations soient effectuées sur les images tirées au hasard. Pour visualiser des segmentations, il faut soit augmenter le nombre d'images affichées (N) ou alors exécuter plusieurs fois la cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320b11e-a855-48c5-bbc6-1cf8d0f7aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize N random samples\n",
    "N = 2\n",
    "for data in random.sample(valid_dataset_dicts, N):\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=valid_metadata, scale=1)\n",
    "    out = visualizer.draw_dataset_dict(data)\n",
    "    imRGBshow(out.get_image())\n",
    "    # print(data[\"file_name\"]) # Print the file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82db0f8-b893-45e5-85ff-7d4f70f5ebf4",
   "metadata": {},
   "source": [
    "# CONFIG DU RESEAU\n",
    "Cette configuration est similaire que pour des utilisations de detectron2 normales.  \n",
    "Il faut cependant changer les configurations par défaut suivantes :\n",
    "- Architecture\n",
    "- Input chargé par le dataloader\n",
    "- Nombre de classes\n",
    "- Poids du réseaux et couches figées\n",
    "- Solveur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa4981-7c80-44af-bf7d-1bb8d7421476",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_architecture_file = '../configs/Segmentation-Z/mask_rcnn_z_50.yaml'\n",
    "\n",
    "# Pour sauvegarder des données d'entrapinement, notamment les poids du réseau entraîné\n",
    "output_directory = \"/local/esaintan/outputs/0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f0c99-0780-4b82-af2f-faf8e0d5a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1      # torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c80c2-36fc-4608-b3b1-73a8cf3f8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de base présente dans ../detectron2/config/defaults.py\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Configuration de l'architecture (depuis le fichier de configuration défini dans config_architecture_file)\n",
    "cfg.merge_from_file(config_architecture_file)\n",
    "\n",
    "# Configuration de l'input : pile. Pour d'autres configuration de pile, il faut soit override les paramètres particuliers configurés dans la fonction, soit écrire une autre fonction.\n",
    "cfg = get_stack_cell_config(cfg)\n",
    "\n",
    "# Configuration du nombre de classes\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n",
    "\n",
    "# Configuration des poids du réseau et des couches figées\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 0      # 0  => aucune couche figée\n",
    "cfg.MODEL.WEIGHTS = \"\"                # \"\" => pas de poids préchargés, ils seront tirés au hasard\n",
    "\n",
    "# Configuration du solveur\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1          # Attention à la taille de la mémoire dont dispose la GPU, doit aussi être un multiple du nombre de GPU\n",
    "if not (cfg.SOLVER.IMS_PER_BATCH % num_gpus):    # Pour être sûr d'être divisible par le nombre de GPU\n",
    "    cfg.SOLVER.IMS_PER_BATCH = (cfg.SOLVER.IMS_PER_BATCH // num_gpus) * num_gpus\n",
    "cfg.SOLVER.MAX_ITER = 100      # Pour test, remettre 10000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "\n",
    "# Configuration du dossier pour sauvegarder les sorties de l'algorithme\n",
    "cfg.OUTPUT_DIR = output_directory\n",
    "\n",
    "# La batch norm  Pas frozen sinon ne s'entraîne pas sur la batch norm Options: FrozenBN, GN, \"SyncBN\", \"BN\"\n",
    "cfg.MODEL.RESNETS.NORM = \"BN\"\n",
    "\n",
    "# Configuration multi GPU\n",
    "cfg.SOLVER.REFERENCE_WORLD_SIZE = num_gpus\n",
    "\n",
    "\n",
    "# La configuration ne pourra plus être modifiée :\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c0293-a281-4c19-b2c5-43f450cbb2d6",
   "metadata": {},
   "source": [
    "# ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd307d-f2be-4995-9f96-cbaaaa4da87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    trainer = DefaultTrainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    # False to begin training from scratch, \n",
    "    # True, takes the specified weights in config, or begin from scratch if no weight specified\n",
    "    # In our case, since we didn't specify weights, trianing will begin from scratch\n",
    "    return trainer.train()\n",
    "\n",
    "launch(train, num_gpus, num_machines=1, dist_url=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c3cc6-b780-47dc-8709-55ca03fe06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2412fc2-06d6-43a8-aba5-b3a58a438cdc",
   "metadata": {},
   "source": [
    "# TEST DU MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b331c73-b56b-49b6-a3af-0efe74ec2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.defrost()\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c35b7138-764a-4a41-9226-b1764f0ceb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/esaintan/.conda/envs/PTI/lib/python3.11/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ae9f1aa-46be-4859-b4b4-ea0cb61687aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cross validation 0, mode test: 15444it [00:00, 49214.63it/s]\n"
     ]
    }
   ],
   "source": [
    "test_metadata = MetadataCatalog.get(dataset_name_test)\n",
    "test_dataset_dicts = DatasetCatalog.get(dataset_name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea55a98-47cd-4fb4-9f00-ea52ee738ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m random\u001b[38;5;241m.\u001b[39msample(test_dataset_dicts, N):\n\u001b[1;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     visualizer \u001b[38;5;241m=\u001b[39m Visualizer(img[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], metadata\u001b[38;5;241m=\u001b[39mtest_metadata, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mdraw_instance_predictions(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/TDSI/PTI_cell/detectron2/engine/defaults.py:220\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_input)\u001b[0m\n\u001b[1;32m    218\u001b[0m         height, width \u001b[38;5;241m=\u001b[39m original_input[z]\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    219\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug\u001b[38;5;241m.\u001b[39mget_transform(original_input[z])\u001b[38;5;241m.\u001b[39mapply_image(original_input[z])\n\u001b[0;32m--> 220\u001b[0m         image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    222\u001b[0m         inputs[z] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: height, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: width}\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Apply pre-processing to image.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "# Faire visualisation pour une stack\n",
    "\n",
    "# Visualize N random samples\n",
    "N = 2\n",
    "for data in random.sample(test_dataset_dicts, N):\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=test_metadata, scale=1)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    # out = visualizer.draw_dataset_dict(data)\n",
    "    imRGBshow(out.get_image())\n",
    "    # print(data[\"file_name\"]) # Print the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf0a61-f030-4cc8-8de1-dcfc791a8be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
